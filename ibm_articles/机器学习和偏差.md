# 机器学习和偏差
观察机器学习模型偏差的影响，探索消除此类偏差的方法

**标签:** 人工智能,机器学习,深度学习

[原文链接](https://developer.ibm.com/zh/articles/machine-learning-and-bias/)

M. Tim Jones

发布: 2019-10-10

* * *

偏差是指对某个人、某个群体或某个事物所持有的一种有失公允的偏见。随着机器学习日渐成为我们日常生活中不可或缺的一部分，问题也随之而来，即机器学习是否也存在偏差？在本文中，我将深入探究这个问题及其产生的影响，并探讨消除机器学习模型偏差的多种方法。

在助力自动驾驶汽车、准确识别 X 光照片中的癌症以及根据过往行为预测我们的兴趣等众多方面，机器学习展现出的潜力都令人叹为观止。但是，机器学习在带来诸多优势的同时，也带来了许多挑战。其中一大挑战就是，机器学习的分类和预测存在偏差。这些偏差并非是善意的。根据机器学习模型产生的决策，这些偏差会导致各种各样的后果。因此，务必要了解机器学习模型中是如何引入偏差的，该如何测试是否存在偏差，以及如何消除偏差。

## 偏差问题

用于评估已定罪罪犯的量刑和假释的工具 (COMPAS)，即是机器学习中存在偏差的一个例子。由于许多监狱人满为患，人们希望通过评估来识别再次犯罪可能性较低的囚犯。随后，仔细审查这些囚犯，看看能否先行释放，以便为后续新入狱罪犯留出空间。通过关于囚犯的大量问题来定义一个风险评分，其中包括某个囚犯的父母是否曾入狱或者其友人或熟人是否曾入狱这样的问题（但不包括种族问题）。

人们已经发现此工具能够成功预测已定罪罪犯成为累犯的可能性，但当在判断公式中引入了种族时，预测就出现了错误。值得指出的是，COMPAS 开发公司后续提供了数据来支持其算法得出的结果，因此陪审团对此仍未有定论，但这也指出了偏差是否存在这个问题。这些方法将受到质疑，并且需要后续提供数据来证明其公平性。

在各种用例中，机器学习被视为人力资源领域的关键工具，从提供培训建议到招聘和其他战术活动，皆有所涉及。2014 年，Amazon 开始开发一套系统来筛选求职者，以此来自动执行根据求职者简历上的文本识别要寻找的关键求职者这一流程。但 Amazon 后来发现，在为工程角色挑选人才时，此算法似乎更偏向于男性而不是女性。在发现此算法缺乏公平性并多次尝试在其中注入公平性之后，Amazon 最终放弃了这套系统。

Google Photos 应用程序可通过识别图片中的对象对图片进行分类。但人们在使用该程序时，发现这里存在某种种族偏见。Amazon 的 Rekognition 是一套商用人脸分析系统，这套系统同样被发现存在性别和种族偏见。

最后一个例子是微软的 Tay Twitter 机器人。Tay 是一个对话式 AI（聊天机器人），通过在 Twitter 上与人们互动来学习。此算法挖掘公共数据来构建对话模型，同时也在 Twitter 上不断从互动中学习。遗憾的是，Tay 经历的互动并非都是正面的，Tay 领悟了现代社会的偏见，这甚至体现在机器模型中，正所谓“种瓜得瓜，种豆得豆”。

## 偏差的影响

无论存在何种偏差，机器学习算法的建议都对个人和群体产生了切实的影响。包含偏差的机器学习模型可以通过自我实现的方式，帮助偏差延续下去。因此，当务之急是要检测出这些模型中的偏差，并尽可能地摒弃这些偏差。

## 偏差的来源

关于偏差的出现，可以简单地将其归结为是数据产生的结果，但其源头却难以捉摸，通常与数据来源、数据内容（它是否包含模型应忽视的元素？）以及模型本身的训练（例如，在模型的分类环境中如何定义好坏）有关。

如果机器学习算法仅仅是根据日间驾驶视频来训练的，那么允许模型在夜间驾驶就会导致悲剧性的结果。这不同于人为偏见，但也证明了确实存在对于手头问题缺乏代表性数据集这一现象。

偏差的出现也可能会是我们所始料不及的。以 Amazon 的招聘工具为例，该模型对于某些求职者的用词进行了罚分，而对另一些求职者的用词则给予了奖分。在这个例子中，遭罚分的用词是女性通常使用的带性别色彩的词汇，而女性在这个数据集中同样缺乏足够的代表性数据。Amazon 的工具主要是利用 10 年间男性的简历进行训练的，这就导致了根据男性间使用的语言而倾向于男性简历这样一种偏差。

即使是人类也可能无意间放大机器学习模型中的偏差。人类的偏见可能是无意识的（也称为 _隐性偏见_），这表示人类甚至可能会在自己毫不知情的情况下引入偏见。

让我们来研究一下如何检测机器学习模型中的偏差，以及如何消除这些偏差。

## 偏差的类型

在机器学习开发领域的诸多领导者所提供的工具中，机器学习数据集和模型的偏差问题同样也屡见不鲜。

检测偏差先从数据集开始。数据集可能无法体现问题空间（如仅使用日间行车数据来训练自动驾驶车辆）。数据集还可能含有可能不应予以考量的数据（例如，个人的种族或性别）。这些分别被称为 _样本偏差_ 和 _偏见偏差_。

由于数据在用于训练或测试机器学习模型之前通常会经过清理，因此还存在 _排除偏差_。当我们移除个人认为不相关的特征时，就会出现这种情况。当收集的训练数据不同于生产期间收集的数据时，就会产生 _测量偏差_。当通过某种特定类型的相机收集数据集，而生产数据却来自于具有不同特征的相机时，就会发生此情况。

最后还有 _算法偏差_，它并非源自用于训练模型的数据，而是来自机器学习模型本身。这包括导致产生不公平结果的模型开发方式或模型训练方式。

## 解决偏差问题

既然我已经为大家提供了偏差的示例和来源，现在就让我们来研究一下如何在机器学习模型中检测并防止偏差。我们将探究谷歌、微软、IBM 的一些解决方案以及其他开源解决方案。

### 谷歌的 What-If Tool

谷歌的 What-If Tool (WIT) 是一个交互式工具，允许用户以直观方式来调查机器学习模型。WIT 现已成为开源 TensorBoard Web 应用程序的一部分，它提供了分析数据集和经过训练的 TensorFlow 模型的功能。
WIT 能够手动编辑来自数据集的示例，并通过关联模型查看更改效果。它还可生成部分依赖关系图，展示更改某项功能时预测结果的相应变化。WIT 可以应用各种公平性条件来分析模型性能（针对群体无意识或公平机会进行优化）。WIT 简单易用，包含各种演示，可帮助用户快速上手。

### IBM AI Fairness 360

IBM 的 [AI Fairness 360](https://aif360.mybluemix.net/) 是用于检测和消除机器学习模型偏差的功能最全面的工具集之一。AI Fairness 360 是一个 [开源工具集](https://github.com/IBM/AIF360)，包含超过 70 种公平性指标和 10 余种纠偏算法，可帮助您检测并消除偏差。纠偏算法包含经过优化的预处理、权重调整、偏差去除正则化项等等。指标包括欧氏距离和曼哈顿距离、统计奇偶差等等。

AI Fairness 360 包含丰富的教程和文档。您还可以使用覆盖三个数据集（包括 COMPAS 累犯数据集）的在线交互演示来浏览偏差指标，然后应用纠偏算法，查看与原始模型的结果比较情况。此工具集专为开源而设计，允许研究人员添加自己的公平性指标和纠偏算法。

IBM 研究人员还在” [Towards Composable Bias Rating of AI Services](https://arxiv.org/pdf/1808.00089.pdf).”中提出了一种面向机器学习模型的偏差评分系统。这里设想的第三方评分系统即用于验证机器学习模型的偏差情况。

### 微软在词嵌入方面的偏差发现

类似于微软在自然界学习的经验，数据集也可能包含偏差。马里兰大学和微软研究院在最近发布的一份题为“ [What are the biases in my word embedding?](https://arxiv.org/pdf/1812.08769.pdf)”的报告中，确立了一种使用众包来识别文字编码（自然语言）中的偏差的方法。 [词嵌入](https://en.wikipedia.org/wiki/Word_embedding) 可在高维空间中通过特征向量来表示文字。随后，这些特征向量支持向量算术运算。这样即可对“男人是国王，女人就是 x”之类的智力游戏进行类推分析。计算 _x_ 的结果为女王，这是一个合理的答案。但观察其他的类推结果会发现，某些方面可能会存在偏差。例如，“男人是计算机程序员，女人就是家庭主妇”即反应了性别偏见。而“父亲是医生，母亲就是护士”也存在同样的问题。

微软证明了能够使用关联测试自动发现文字编码中的偏差。这受到了内隐联想测验 (IAT) 的启发，后者广泛用于测量人为偏见。通过使用众包来确认偏差，这一发现结果随后得到了验证。

通过这个过程，可以帮助词嵌入用户减少此数据集的偏差。

### 其他框架

偏差已成为过去几年里最热门的机器学习研究领域之一，而用于检测模型偏差和纠偏的框架也层出不穷。

Local Interpretable Model-Agnostic Explanations (Lime) 可用于理解模型提供特定预测的原因。Lime 适用于任何模型，可为给定预测提供人类可理解的解释。

FairML 是一个工具箱，可通过量化模型输入的相对重要性来审计预测模型。随后，这种相对重要性可用于评估模型的公平性。FairML 也适用于任何黑箱模型。

## 人机回圈

在许多情况下，机器学习模型都是一个黑箱。您可以为其提供输入并观察其输出，但这些模型将输入映射到输出的方式都隐藏在经过训练的模型中。可解释的模型可以帮助公开机器学习模型获得结论的方式，但在这些模型得到普遍应用之前，另一种替代方法就是 _人机回圈_。

[人机回圈](https://blog.algorithmia.com/machine-learning-with-human-in-the-loop/) 是一种混合模型，可将传统机器学习与监视机器学习模型结果的人类联系在一起。这使人们能够观察何时采用算法或何时出现数据集偏差。回想一下微软是如何使用众包来验证其词嵌入偏差发现结果的，这也表明了人机回圈是一个可供采用的实用混合模型。

## 结束语

公平性是一把双刃剑，而对于公平性的数学定义，人类至今仍未能达成共识。我们可以采用五分之四规则或其他平等措施，但缺陷很快就会随之出现。

在我们能够构建完全透明且可解释的模型之前，我们仍将需要依赖各种工具集来测量机器学习模型中的偏差并进行纠偏。值得庆幸的是，这些工具集功能丰富，包含了大量公平性指标和纠偏算法。

本文翻译自： [Machine learning and bias](https://developer.ibm.com/articles/machine-learning-and-bias/)（2019-08-27）